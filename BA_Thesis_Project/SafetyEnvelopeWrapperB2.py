import gymnasium as gym
import numpy as np
from stable_baselines3.her import GoalSelectionStrategy

from BA_Thesis_Project.SafetyEnvelopeWrapperB1 import SafetyEnvelopeWrapperB1

MAX_ADVANCE_ROBOT = 0.1
MAX_ROTATION_ROBOT = 1.57
# below this threshold, safe
ALERT_HUMAN_SAFE_THRESHOLD = 0.3
# per step for high alert
ALERT_HUMAN_HIGH_PENALTY = -0.8
# mild penalty per step for any alertness_Human >0
ALERT_HUMAN_BASE_PENALTY = -0.2
# Wall alertness shaping (to avoid wall hugging), per step=ALERT_WALL_PENALTY*alertWall
ALERT_WALL_PENALTY = -0.6

# ALERT_NEAR_HUMAN_THRESHOLD=0.4
# FAR_ROTATION_PENALTY=-0.2
CURVE_PENALTY = -1
# NEAR_ROTATION_BONUS=0.5

NEAR_GOAL=4.0

JERK_ROTATION_PENALTY = -0.3

# forward speed bonus factor
SPEED_BONUS_SCALE = 4.0
# never kill speed bonus completely
MIN_SPEED_FACTOR = 0.3


class SafetyEnvelopeWrapperB2(SafetyEnvelopeWrapperB1):
    def __init__(self, env: gym.Env, ):
        super().__init__(env)

    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)

        # forward speed, side speed from original action generated by the PPO -utility focused component
        forward_speed = max(0.0, action[0]) * MAX_ADVANCE_ROBOT
        rotation_speed = abs(action[2]) * MAX_ROTATION_ROBOT

        # USE ALERTNESS HUMAN TO MODULATE SPEED
        alertness_human = self.alertnessHuman

        # far from human-> big speed factor
        # near human -> smaller bonus , but not 0 to prevent freeze near human, encourage robot to move faster when not near human, move slower otherwise

        distance_to_goal = np.sqrt((self.env.get_wrapper_attr("robot").goal_x - self.env.get_wrapper_attr("robot").x) ** 2 + (self.env.get_wrapper_attr("robot").goal_y - self.env.get_wrapper_attr("robot").y) ** 2)
        if  distance_to_goal>NEAR_GOAL:
            speed_factor = MIN_SPEED_FACTOR + (1.0 - MIN_SPEED_FACTOR) * (1.0 - alertness_human)
            speed_bonus = SPEED_BONUS_SCALE * forward_speed * speed_factor
            # max = 4*0.1*1=0.4
            reward += speed_bonus

        # PENALISE ALERTNESS HUMAN -> ANTICIPATION
        # Intuition, if robot keep walking in a way that human alertness slowly increase, it gets negative every step-> learn to
        # deviate ealier so that alerthuman stay low
        # small penalty for any alertness
        reward += ALERT_HUMAN_BASE_PENALTY * alertness_human
        # extra penalty if alertness is high (close to d_min), tell the robot to change path early,
        high_alertHuman = max(0.0, (alertness_human - ALERT_HUMAN_SAFE_THRESHOLD) ** 2)
        reward += ALERT_HUMAN_HIGH_PENALTY * high_alertHuman
        # add jerk penalty, turning is allowed, flip flopping is expensive, aim to remove jitter
        if self.prev_action is not None:
            delta_rotation = abs(action[2] - self.prev_action[2])
            reward += JERK_ROTATION_PENALTY * delta_rotation
        self.prev_action = action

        # rotation_far_penalty=FAR_ROTATION_PENALTY*rotation_speed*(1-alertness_human), do not penalize rotation when forward speed is small
        if alertness_human < 0.3 and forward_speed > 0.03:
            reward += CURVE_PENALTY * forward_speed * rotation_speed

        # PENALIZE WALL ALERTNESS -> prevent wall hugging
        alertness_wall = self.alertnessWall
        reward += ALERT_WALL_PENALTY * alertness_wall

        return obs, reward, terminated, truncated, info





